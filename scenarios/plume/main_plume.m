% bare bones example of use of the QRSim() simulator
% with the plume scenario
%
% in this scenario one or more plumes (that might evolve over time) are present in the flight area
% an helicopter agent is equipped with a sensor that measures the concentration of smoke. 
% The plume follows a known model but with unknown parameter values. 
% The objective is to provide a smoke concentration estimate cT at some prespecified time T

clear all
close all

% include simulator
addpath(['..',filesep,'..',filesep,'sim']);
addpath(['..',filesep,'..',filesep,'controllers']);

% create simulator object
qrsim = QRSim();

% load task parameters and do housekeeping
%state = qrsim.init('TaskPlumeSingleSourceGaussian');
%state = qrsim.init('TaskPlumeSingleSourceGaussianDispersion');
%state = qrsim.init('TaskPlumeMultiSourceGaussianDispersion');
%state = qrsim.init('TaskPlumeMultiHeliMultiSourceGaussianDispersion');
%state = qrsim.init('TaskPlumeSingleSourceGaussianPuffDispersion');
%state = qrsim.init('TaskPlumeMultiSourceGaussianPuffDispersion');
state = qrsim.init('TaskPlumeMultiHeliMultiSourceGaussianPuffDispersion');


% create a 2 x helicopters matrix of control inputs
% column i will contain the 2D NED velocity [vx;vy] in m/s for helicopter i
U = zeros(2,state.task.numUAVs);
tstart = tic;

hf = figure(2);
hp = plot(0,0);

plumeMeas = zeros(1,state.task.durationInSteps);

if(state.display3dOn)
    fprintf('By default when the display is on, the task run at real time speed\n');
    fprintf('in order to allow matlab some time to run its plot refresh thread\n'); 
end

% run the scenario and at every timestep generate a control
% input for each of the helicopters
for i=1:state.task.durationInSteps,
    tloop=tic;
    
    % a basic policy in which the helicopter(s) moves around 
    % at the max velocity changing direction every once in a while
    
    if(rem(i-1,10)==0)
        for j=1:state.task.numUAVs,        
	
            % random velocity direction
            u = rand(2,1)-[0.5;0.5];        

            % scale by the max allowed velocity
            U(:,j) = state.task.velPIDs{j}.maxv*(u/norm(u));
        end
    end
    
    % step simulator
    qrsim.step(U);
    
    % get plume measurement
    plumeMeas(i)=state.platforms{1}.getPlumeSensorOutput();
    
%     positions = state.task.getLocations();
%     samples = state.environment.area.getSamples(positions);
%     set(0,'CurrentFigure',hf)
%     t = (1:size(positions,2));
%     set(hp,'Xdata',t);
%     set(hp,'Ydata',samples);
%     axis([0 300 0 2]);
 %   t = (1:i)*state.task.dt;
%    set(hp,'Xdata',t);
%    set(hp,'Ydata',plumeMeas(1:i));
        
    if(state.display3dOn)
        % wait so to run in real time
        % this can be commented out obviously
        wait = max(0,state.task.dt-toc(tloop));
        pause(wait);
    end
end

% query at what locations we need to make predictions of concentration
positions = state.task.getLocations();

% query how many samples we need per location 
samplesPerLocation = state.task.getSamplesPerLocation();

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% for the purpouse of demonstration, we CHEAT and use samples
%%% generated by the environment itself (i.e. perfect estimation)
samples = zeros(samplesPerLocation,size(positions,2));
samples = state.environment.area.getReferenceSamples();
%%% in a real experiment getReferenceSamples should not be used and
%%% the samples should come from the and model that the agent has
%%% somehow estimated
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% set the samples so that the task can compute a reward
state.task.setSamples(samples);

% get final reward, this works only after the samples have been set.
% reminder: a large negative final reward (-1000) is returned in case of
% collisions or in case of any uav going outside the flight area
fprintf('final reward: %f\n',qrsim.reward());

elapsed = toc(tstart);

fprintf('running %d times real time\n',(state.task.durationInSteps*state.task.dt)/elapsed);
