% bare bones example of use of the QRSim() simulator
% with the plume scenario
%
% in this scenario one or more plumes (that might evolve over time) are present in the flight area
% an helicopter agent is equipped with a sensor that measures the concentration of smoke. 
% The plume follows a known model but with unknown parameter values. 
% The objective is to provide a smoke concentration estimate cT at some prespecified time T

clear all
close all

% include simulator
addpath(['..',filesep,'..',filesep,'sim']);
addpath(['..',filesep,'..',filesep,'controllers']);

% create simulator object
qrsim = QRSim();

% load task parameters and do housekeeping
state = qrsim.init('TaskPlumeSingleSourceGaussian');
%state = qrsim.init('TaskPlumeSingleSourceGaussianDispersion');
%state = qrsim.init('TaskPlumeMultiSourceGaussianDispersion');
%state = qrsim.init('TaskPlumeMultiHeliMultiSourceGaussianDispersion');
%state = qrsim.init('TaskPlumeSingleSourceGaussianPuffDispersion');
%state = qrsim.init('TaskPlumeMultiSourceGaussianPuffDispersion');
%state = qrsim.init('TaskPlumeMultiHeliMultiSourcePuffDispersion');


% create a 2 x helicopters matrix of control inputs
% column i will contain the 2D NED velocity [vx;vy] in m/s for helicopter i
U = zeros(2,state.task.numUAVs);
tstart = tic;

hf = figure(2);
hp = plot(0,0);

plumeMeas = zeros(1,state.task.durationInSteps);

% run the scenario and at every timestep generate a control
% input for each of the helicopters
for i=1:state.task.durationInSteps,
    tloop=tic;
    
    % a basic policy in which the helicopter(s) moves around 
    % at the max velocity changing direction every once in a while
    
    if(rem(i-1,10)==0)
        for j=1:state.task.numUAVs,        
	
            % random velocity direction
            u = rand(2,1)-[0.5;0.5];        

            % scale by the max allowed velocity
            U(:,j) = state.task.velPIDs{j}.maxv*(u/norm(u));
        end
    end
    
    % step simulator
    qrsim.step(U);
    
    % get plume measurement
    plumeMeas(i)=state.platforms{1}.getPlumeSensorOutput();
    
    set(0,'CurrentFigure',hf)
    t = (1:i)*state.task.dt;
    set(hp,'Xdata',t);
    set(hp,'Ydata',plumeMeas(1:i));
        
    % wait so to run in real time
    % this can be commented out obviously
    wait = max(0,state.task.dt-toc(tloop));
    pause(wait);
end

% query at what locations we need to make predictions of concentration
positions = state.task.getLocations();

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% for the purpouse of demonstration, we CHEAT and use samples
%%% generated by the environment itself (i.e. perfect estimation)
samples = state.environment.area.getSamples(positions);
%%% in a real setting the samples should come from the and model that
%%% the agent has somehow estimated
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% set the samples so that the task can compute a reward
state.task.setSamples(samples);

% get final reward, this works only after the samples have been set.
% reminder: a large negative final reward (-1000) is returned in case of
% collisions or in case of any uav going outside the flight area
fprintf('final reward: %f\n',qrsim.reward());

elapsed = toc(tstart);

fprintf('running %d times real time\n',(state.task.durationInSteps*state.DT)/elapsed);
